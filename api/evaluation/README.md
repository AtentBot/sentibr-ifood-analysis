# ðŸŽ¯ FASE 6 - EVAL E LLM INTEGRATION

Sistema completo de avaliaÃ§Ã£o e integraÃ§Ã£o com LLMs para o SentiBR.

## ðŸ“‹ O Que Foi Criado

### âœ… **Evaluation Suite**
- Framework completo de avaliaÃ§Ã£o de modelos
- MÃ©tricas clÃ¡ssicas (Accuracy, Precision, Recall, F1)
- Confusion Matrix e visualizaÃ§Ãµes
- AnÃ¡lise detalhada de erros
- RelatÃ³rios em JSON e texto

### âœ… **LLM-as-Judge**
- AvaliaÃ§Ã£o qualitativa com GPT-4o-mini
- ComparaÃ§Ã£o BERT vs GPT
- IdentificaÃ§Ã£o de casos edge
- AnÃ¡lise por aspectos (comida, entrega, serviÃ§o, preÃ§o)
- ExplicaÃ§Ãµes detalhadas

### âœ… **IntegraÃ§Ã£o OpenAI**
- Cliente OpenAI configurado
- Rate limiting e error handling
- Tracking de custos e tokens
- Suporte a batch processing

---

## ðŸ—ï¸ Estrutura de Arquivos

```
src/evaluation/
â”œâ”€â”€ __init__.py              # MÃ³dulo Python
â”œâ”€â”€ eval_suite.py            # Framework de avaliaÃ§Ã£o
â”œâ”€â”€ llm_judge.py             # LLM-as-Judge
â””â”€â”€ README.md                # Esta documentaÃ§Ã£o

logs/evaluation/             # Resultados de avaliaÃ§Ã£o
logs/llm_judge/              # Resultados do LLM Judge
```

---

## ðŸš€ Quick Start

### 1ï¸âƒ£ **Configurar OpenAI API Key**

```bash
export OPENAI_API_KEY='your-api-key-here'
```

Ou crie um arquivo `.env`:

```bash
OPENAI_API_KEY=your-api-key-here
```

### 2ï¸âƒ£ **Instalar DependÃªncias**

```bash
pip install openai pandas matplotlib seaborn tqdm
```

### 3ï¸âƒ£ **Executar AvaliaÃ§Ã£o BÃ¡sica**

```python
from src.evaluation import ModelEvaluator
import numpy as np

# Criar dados de teste
y_true = np.array([0, 1, 2, 0, 1, 2])
y_pred = np.array([0, 1, 2, 1, 1, 2])

# Criar avaliador
evaluator = ModelEvaluator(
    model_name="BERT Fine-tuned",
    label_names=['negativo', 'neutro', 'positivo']
)

# Avaliar
result = evaluator.evaluate(y_true, y_pred)

# Mostrar resultados
print(result.summary())

# Salvar
result.to_json("evaluation_results.json")
```

### 4ï¸âƒ£ **Usar LLM-as-Judge**

```python
from src.evaluation import LLMJudge

# Criar judge
judge = LLMJudge(model="gpt-4o-mini")

# Julgar uma prediÃ§Ã£o
result = judge.judge_single(
    text="A comida estava deliciosa!",
    bert_pred="positivo"
)

# Ver resultado
print(result.explanation)
print(f"Acordo com BERT: {result.agreement_with_bert}")
print(f"ConfianÃ§a: {result.confidence}")
```

---

## ðŸ“Š Exemplo Completo

### AvaliaÃ§Ã£o + LLM Judge

```python
import pandas as pd
from src.evaluation import ModelEvaluator, LLMJudge

# 1. Carregar dados de teste
test_data = pd.read_csv("data/processed/test.csv")

# 2. Fazer prediÃ§Ãµes com BERT
from src.api.inference import SentimentPredictor
predictor = SentimentPredictor()

predictions = []
for text in test_data['text']:
    pred = predictor.predict(text)
    predictions.append(pred['sentiment'])

# 3. Avaliar com mÃ©tricas clÃ¡ssicas
evaluator = ModelEvaluator(model_name="BERT Fine-tuned")
result = evaluator.evaluate(
    y_true=test_data['label'].values,
    y_pred=predictions,
    texts=test_data['text'].tolist()
)

print(result.summary())
evaluator.plot_confusion_matrix(test_data['label'].values, predictions)

# 4. Avaliar com LLM Judge (sample de 100)
judge = LLMJudge()
llm_results, metrics = judge.judge_batch(
    texts=test_data['text'].tolist()[:100],
    bert_preds=predictions[:100],
    max_samples=100
)

judge.print_summary(metrics)

# 5. Salvar tudo
result.to_json("logs/evaluation/bert_evaluation.json")
```

---

## ðŸ“ˆ MÃ©tricas Calculadas

### **Evaluation Suite**

1. **Overall Metrics**
   - Accuracy
   - Precision (macro e weighted)
   - Recall (macro e weighted)
   - F1-Score (macro e weighted)

2. **Per-Class Metrics**
   - Precision por classe
   - Recall por classe
   - F1-Score por classe
   - Support (nÃºmero de samples)

3. **Error Analysis**
   - Total de erros
   - Taxa de erro
   - DistribuiÃ§Ã£o de erros por tipo
   - Exemplos de erros

4. **VisualizaÃ§Ãµes**
   - Confusion Matrix
   - ComparaÃ§Ã£o entre modelos
   - DistribuiÃ§Ã£o de prediÃ§Ãµes

### **LLM Judge**

1. **Agreement Rates**
   - Taxa de acordo com BERT
   - Taxa de acordo com GPT
   - AnÃ¡lise de discordÃ¢ncias

2. **Edge Cases**
   - IdentificaÃ§Ã£o de casos difÃ­ceis
   - Taxa de edge cases
   - AnÃ¡lise de ambiguidade

3. **Aspect Analysis**
   - Sentimento por aspecto (comida, entrega, etc)
   - DistribuiÃ§Ã£o de aspectos mencionados
   - CorrelaÃ§Ã£o entre aspectos

4. **Confidence**
   - ConfianÃ§a mÃ©dia do LLM
   - DistribuiÃ§Ã£o de confianÃ§a
   - CorrelaÃ§Ã£o com dificuldade

---

## ðŸŽ¯ Casos de Uso

### 1. **Avaliar Modelo Novo**

```python
evaluator = ModelEvaluator(model_name="BERTimbau v2")
result = evaluator.evaluate(y_true, y_pred, texts)
evaluator.plot_confusion_matrix(y_true, y_pred)
report = evaluator.generate_report(result)
```

### 2. **Comparar Modelos**

```python
# Avaliar BERT
bert_result = evaluator_bert.evaluate(y_true, y_pred_bert)

# Avaliar GPT
gpt_result = evaluator_gpt.evaluate(y_true, y_pred_gpt)

# Comparar
evaluator.plot_metrics_comparison([bert_result, gpt_result])
```

### 3. **Encontrar Casos DifÃ­ceis**

```python
judge = LLMJudge()
results, metrics = judge.judge_batch(texts, bert_preds)

# Filtrar edge cases
edge_cases = [r for r in results if r.is_edge_case]
print(f"Encontrados {len(edge_cases)} casos difÃ­ceis")

for case in edge_cases[:5]:
    print(f"Text: {case.text}")
    print(f"Explanation: {case.explanation}\n")
```

### 4. **AnÃ¡lise de Aspectos**

```python
results, _ = judge.judge_batch(texts, bert_preds)

# Agregar por aspecto
aspect_sentiments = {
    'food': [],
    'delivery': [],
    'service': [],
    'price': []
}

for result in results:
    for aspect, sentiment in result.aspects.items():
        if sentiment != 'nÃ£o mencionado':
            aspect_sentiments[aspect].append(sentiment)

# Mostrar distribuiÃ§Ã£o
for aspect, sentiments in aspect_sentiments.items():
    print(f"\n{aspect.upper()}:")
    print(f"  Positivo: {sentiments.count('positivo')}")
    print(f"  Neutro:   {sentiments.count('neutro')}")
    print(f"  Negativo: {sentiments.count('negativo')}")
```

---

## ðŸ’° Custos do LLM Judge

### **GPT-4o-mini Pricing (Nov 2024)**

- **Input**: $0.15 / 1M tokens
- **Output**: $0.60 / 1M tokens

### **Estimativas**

Para 100 reviews de ~100 palavras cada:

- **Tokens por review**: ~300-500 tokens
- **Total de tokens**: 30-50k tokens
- **Custo estimado**: **$0.02 - $0.04**

Para 1000 reviews:

- **Custo estimado**: **$0.20 - $0.40**

### **Dicas para Reduzir Custos**

1. **Sample estrategicamente**: NÃ£o precisa avaliar todos os casos
2. **Use cache**: Reutilize avaliaÃ§Ãµes quando possÃ­vel
3. **Batch processing**: Processe em lotes para economia
4. **Filtre por confianÃ§a**: Avalie apenas casos com baixa confianÃ§a do BERT

---

## ðŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### **Customizar LLM Judge**

```python
judge = LLMJudge(
    model="gpt-4o-mini",           # Modelo a usar
    temperature=0.1,               # Criatividade (0-1)
    max_tokens=1000,               # MÃ¡x tokens na resposta
    output_dir=Path("custom/dir")  # Dir para salvar
)
```

### **Customizar Prompts**

```python
# Editar prompts em llm_judge.py
LLMJudge.SYSTEM_PROMPT = "Seu prompt customizado..."
LLMJudge.USER_PROMPT_TEMPLATE = "Template customizado..."
```

### **Adicionar MÃ©tricas Customizadas**

```python
# Em eval_suite.py
def custom_metric(y_true, y_pred):
    # Sua lÃ³gica aqui
    return score

# Adicionar ao evaluator
evaluator.custom_metrics['my_metric'] = custom_metric
```

---

## ðŸ“Š Outputs Gerados

### **Evaluation Suite**

```
logs/evaluation/
â”œâ”€â”€ bert_evaluation.json              # Resultados em JSON
â”œâ”€â”€ evaluation_report.txt             # RelatÃ³rio em texto
â”œâ”€â”€ confusion_matrix_BERT.png         # Matriz de confusÃ£o
â””â”€â”€ metrics_comparison.png            # ComparaÃ§Ã£o entre modelos
```

### **LLM Judge**

```
logs/llm_judge/
â”œâ”€â”€ judgments_20241106_143022.json    # Julgamentos individuais
â””â”€â”€ metrics_20241106_143022.json      # MÃ©tricas agregadas
```

---

## ðŸŽ“ Conceitos AvanÃ§ados

### **LLM-as-Judge**

O conceito de usar LLMs para avaliar outputs de outros modelos Ã© poderoso porque:

1. **AvaliaÃ§Ã£o Qualitativa**: Vai alÃ©m de mÃ©tricas numÃ©ricas
2. **Nuances**: Captura sutilezas que mÃ©tricas nÃ£o capturam
3. **Explicabilidade**: Fornece razÃµes para as avaliaÃ§Ãµes
4. **Flexibilidade**: Pode avaliar mÃºltiplos aspectos
5. **Escalabilidade**: Mais barato que avaliaÃ§Ã£o humana

### **Quando Usar LLM Judge**

âœ… **Use quando:**
- Precisa entender WHY o modelo errou
- Quer avaliar qualidade qualitativa
- Tem budget para API calls
- Precisa identificar edge cases
- Quer anÃ¡lise de mÃºltiplos aspectos

âŒ **NÃ£o use quando:**
- Precisa apenas de mÃ©tricas quantitativas
- Budget Ã© muito limitado
- Precisa de avaliaÃ§Ã£o instantÃ¢nea
- Dataset Ã© muito grande (>10k samples)

### **BERT vs GPT Trade-offs**

| Aspecto | BERT Fine-tuned | GPT-4o-mini |
|---------|----------------|-------------|
| **LatÃªncia** | < 100ms | ~1-2s |
| **Custo** | $0 (apÃ³s treino) | $0.20-0.40 / 1k reviews |
| **PrecisÃ£o** | 92-95% | 94-97% |
| **Contexto** | Limitado (512 tokens) | Extenso (128k tokens) |
| **Explicabilidade** | Baixa | Alta |
| **CustomizaÃ§Ã£o** | Alta (fine-tuning) | MÃ©dia (prompts) |
| **ProduÃ§Ã£o** | Ideal para alto volume | Ideal para casos crÃ­ticos |

### **EstratÃ©gia HÃ­brida (Recomendada)**

```python
# 1. BERT para todos os casos
bert_pred = bert_model.predict(text)

# 2. Se confianÃ§a baixa, usar GPT
if bert_pred['confidence'] < 0.70:
    gpt_pred = gpt_model.predict(text)
    final_pred = gpt_pred
else:
    final_pred = bert_pred

# 3. LLM Judge em sample aleatÃ³rio para monitoramento
if random.random() < 0.01:  # 1% de sample
    judge.judge_single(text, final_pred)
```

---

## ðŸ§ª Testing

### **Testar Evaluation Suite**

```bash
python -m pytest tests/unit/test_eval_suite.py -v
```

### **Testar LLM Judge**

```bash
# Requer OPENAI_API_KEY
export OPENAI_API_KEY='your-key'
python src/evaluation/llm_judge.py
```

---

## ðŸ“š ReferÃªncias

### **Papers**

- [Constitutional AI](https://arxiv.org/abs/2212.08073) - Anthropic
- [Judging LLM-as-a-Judge](https://arxiv.org/abs/2306.05685)
- [BERT for Sentiment Analysis](https://arxiv.org/abs/1810.04805)

### **Recursos**

- [OpenAI API Docs](https://platform.openai.com/docs)
- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)
- [MLflow Evaluation](https://mlflow.org/docs/latest/models.html#model-evaluation)

### **Exemplos de Prompts**

- [Awesome Prompts](https://github.com/f/awesome-chatgpt-prompts)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

---

## ðŸŽ¯ PrÃ³ximos Passos

ApÃ³s completar a Fase 6:

1. **FASE 7**: Docker + Deploy Completo
2. **FASE 8**: Testes (Unit + Integration + Load)
3. **FASE 9**: DocumentaÃ§Ã£o Final

### **Melhorias Futuras**

- [ ] Suporte a outros LLMs (Claude, Gemini)
- [ ] Active Learning com LLM Judge
- [ ] Dashboard interativo de avaliaÃ§Ã£o
- [ ] A/B testing framework
- [ ] Continuous evaluation pipeline

---

## ðŸŽ‰ Resumo

âœ… **Evaluation Suite** completo com mÃ©tricas clÃ¡ssicas  
âœ… **LLM-as-Judge** com GPT-4o-mini  
âœ… **AnÃ¡lise de aspectos** (comida, entrega, etc)  
âœ… **ComparaÃ§Ã£o BERT vs GPT** side-by-side  
âœ… **IdentificaÃ§Ã£o de edge cases**  
âœ… **ExplicaÃ§Ãµes detalhadas**  
âœ… **VisualizaÃ§Ãµes** e relatÃ³rios  
âœ… **Tracking de custos** e tokens  

**FASE 6 - 100% COMPLETA!** ðŸš€

---

**Desenvolvido com â¤ï¸ para o desafio tÃ©cnico de IA SÃªnior**

ðŸŽ¯ Evaluation + ðŸ¤– LLM Integration = ðŸ’ª Production-Ready AI!
