"""
SentiBR - Avalia√ß√£o do Modelo
Testa a acur√°cia e performance do modelo BERT
"""
import streamlit as st
import pandas as pd
import plotly.figure_factory as ff
import plotly.express as px

st.set_page_config(page_title="Avalia√ß√£o", page_icon="üîé", layout="wide")

st.title("üîé Avalia√ß√£o do Modelo")
st.markdown("Teste a acur√°cia e performance do modelo BERT")

# Tabs
tab1, tab2, tab3 = st.tabs(["üìä M√©tricas", "üéØ Teste Manual", "üìà Hist√≥rico"])

with tab1:
    st.subheader("üìä M√©tricas de Performance")
    
    # M√©tricas principais
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Acur√°cia", "92.3%", "+1.2%")
    
    with col2:
        st.metric("Precision", "91.5%", "+0.8%")
    
    with col3:
        st.metric("Recall", "93.1%", "+1.5%")
    
    with col4:
        st.metric("F1-Score", "92.3%", "+1.1%")
    
    st.markdown("---")
    
    # Matriz de confus√£o
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Matriz de Confus√£o")
        
        # Dados de exemplo
        z = [
            [2450, 120, 80],   # Positivo
            [95, 1820, 115],   # Neutro
            [88, 102, 1910]    # Negativo
        ]
        
        x = ['Positivo', 'Neutro', 'Negativo']
        y = ['Positivo', 'Neutro', 'Negativo']
        
        fig = ff.create_annotated_heatmap(
            z,
            x=x,
            y=y,
            annotation_text=[[str(val) for val in row] for row in z],
            colorscale='Blues'
        )
        
        fig.update_layout(
            xaxis_title="Predi√ß√£o",
            yaxis_title="Real",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("M√©tricas por Classe")
        
        # Dados por classe
        class_metrics = pd.DataFrame({
            'Classe': ['Positivo', 'Neutro', 'Negativo'],
            'Precision': [0.927, 0.891, 0.910],
            'Recall': [0.924, 0.897, 0.910],
            'F1-Score': [0.926, 0.894, 0.910],
            'Suporte': [2650, 2030, 2100]
        })
        
        st.dataframe(class_metrics, hide_index=True, use_container_width=True)
        
        # Gr√°fico de barras
        fig = px.bar(
            class_metrics,
            x='Classe',
            y=['Precision', 'Recall', 'F1-Score'],
            barmode='group',
            title='M√©tricas por Classe'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # Curvas
    st.markdown("---")
    st.subheader("Curvas de Performance")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### Curva ROC")
        
        # Dados simulados para curva ROC
        import numpy as np
        
        fpr = np.linspace(0, 1, 100)
        tpr_pos = 1 - np.exp(-5 * fpr)
        tpr_neu = 1 - np.exp(-4 * fpr)
        tpr_neg = 1 - np.exp(-4.5 * fpr)
        
        fig = px.line()
        fig.add_scatter(x=fpr, y=tpr_pos, name='Positivo (AUC=0.94)', mode='lines')
        fig.add_scatter(x=fpr, y=tpr_neu, name='Neutro (AUC=0.89)', mode='lines')
        fig.add_scatter(x=fpr, y=tpr_neg, name='Negativo (AUC=0.91)', mode='lines')
        fig.add_scatter(x=[0, 1], y=[0, 1], name='Baseline', mode='lines', line=dict(dash='dash'))
        
        fig.update_layout(
            xaxis_title='False Positive Rate',
            yaxis_title='True Positive Rate',
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### Distribui√ß√£o de Confian√ßa")
        
        # Dados simulados de confian√ßa
        confidence_data = pd.DataFrame({
            'Confian√ßa': np.concatenate([
                np.random.normal(0.85, 0.1, 500),
                np.random.normal(0.7, 0.15, 300),
                np.random.normal(0.6, 0.2, 200)
            ]),
            'Correto': ['Sim'] * 500 + ['N√£o'] * 500
        })
        
        fig = px.histogram(
            confidence_data,
            x='Confian√ßa',
            color='Correto',
            nbins=30,
            title='Distribui√ß√£o de Confian√ßa'
        )
        
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)

with tab2:
    st.subheader("üéØ Teste Manual")
    st.markdown("Teste o modelo com seus pr√≥prios dados")
    
    # Upload de dados de teste
    st.markdown("#### Upload de Dados de Teste")
    
    st.info("""
    **Formato esperado:**
    - Coluna `text`: Texto do review
    - Coluna `label`: Sentimento real (positive, neutral, negative)
    
    ```csv
    text,label
    "Comida deliciosa!",positive
    "P√©ssimo atendimento",negative
    ```
    """)
    
    test_file = st.file_uploader("Arquivo CSV de Teste", type=['csv'])
    
    if test_file:
        try:
            df_test = pd.read_csv(test_file)
            
            st.success(f"‚úÖ {len(df_test)} exemplos carregados")
            
            with st.expander("Preview"):
                st.dataframe(df_test.head())
            
            if st.button("üöÄ Executar Avalia√ß√£o", type="primary"):
                with st.spinner("Avaliando..."):
                    # Aqui faria a avalia√ß√£o real
                    st.success("Avalia√ß√£o conclu√≠da!")
                    
                    # Resultados simulados
                    st.metric("Acur√°cia", "91.2%")
                    st.metric("Total Corretos", "912 / 1000")
        
        except Exception as e:
            st.error(f"Erro ao processar arquivo: {e}")
    
    else:
        # Exemplo manual
        st.markdown("---")
        st.markdown("#### Teste Individual")
        
        col1, col2 = st.columns(2)
        
        with col1:
            test_text = st.text_area("Texto do Review", height=100)
        
        with col2:
            true_label = st.selectbox(
                "Sentimento Real",
                options=['positive', 'neutral', 'negative']
            )
        
        if st.button("Testar", type="primary"):
            if test_text:
                # Simula√ß√£o de predi√ß√£o
                st.info(f"**Predi√ß√£o**: positive (92%)")
                st.info(f"**Real**: {true_label}")
                
                if "positive" == true_label:
                    st.success("‚úÖ Correto!")
                else:
                    st.error("‚ùå Incorreto")

with tab3:
    st.subheader("üìà Hist√≥rico de Performance")
    
    # Dados hist√≥ricos simulados
    import datetime
    
    dates = pd.date_range(end=datetime.datetime.now(), periods=30, freq='D')
    
    history_data = pd.DataFrame({
        'Data': dates,
        'Acur√°cia': np.random.normal(0.92, 0.02, 30).clip(0.85, 0.95),
        'F1-Score': np.random.normal(0.91, 0.02, 30).clip(0.84, 0.94),
        'Lat√™ncia (ms)': np.random.normal(87, 15, 30).clip(50, 150)
    })
    
    # Gr√°fico de evolu√ß√£o
    fig = px.line(
        history_data,
        x='Data',
        y=['Acur√°cia', 'F1-Score'],
        title='Evolu√ß√£o das M√©tricas (√öltimos 30 dias)'
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Gr√°fico de lat√™ncia
    fig2 = px.line(
        history_data,
        x='Data',
        y='Lat√™ncia (ms)',
        title='Lat√™ncia ao Longo do Tempo'
    )
    
    st.plotly_chart(fig2, use_container_width=True)
    
    # Tabela de hist√≥rico
    st.markdown("#### Hist√≥rico Detalhado")
    st.dataframe(
        history_data.sort_values('Data', ascending=False).head(10),
        hide_index=True,
        use_container_width=True
    )

# Informa√ß√µes
with st.expander("‚ÑπÔ∏è Sobre as M√©tricas"):
    st.markdown("""
    ### M√©tricas de Classifica√ß√£o
    
    - **Acur√°cia**: Percentual de predi√ß√µes corretas
    - **Precision**: Dentre os preditos como X, quantos realmente s√£o X
    - **Recall**: Dentre os que s√£o X, quantos foram preditos como X
    - **F1-Score**: M√©dia harm√¥nica entre Precision e Recall
    
    ### Matriz de Confus√£o
    
    - Diagonal principal: Predi√ß√µes corretas
    - Fora da diagonal: Erros do modelo
    - Linha: Classe real
    - Coluna: Classe predita
    
    ### Curva ROC
    
    - AUC (Area Under Curve): Quanto maior, melhor
    - Ideal: AUC = 1.0
    - Baseline: AUC = 0.5
    """)
