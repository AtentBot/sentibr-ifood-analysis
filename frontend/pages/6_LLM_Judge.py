"""
SentiBR - LLM as a Judge
GPT-4 avalia a qualidade das predi√ß√µes do BERT
"""
import streamlit as st
import time

st.set_page_config(page_title="LLM Judge", page_icon="ü§ñ", layout="wide")

st.title("ü§ñ LLM as a Judge")
st.markdown("GPT-4o-mini avalia a qualidade das predi√ß√µes do BERT")

# Explica√ß√£o
st.info("""
**Como funciona:**

1. BERT faz a predi√ß√£o inicial (r√°pido, ~100ms)
2. GPT-4o-mini avalia se a predi√ß√£o foi correta (lento, ~2s)
3. GPT justifica sua avalia√ß√£o com racioc√≠nio detalhado
4. Sistema aprende com feedbacks para melhorar

**Objetivo**: Validar e melhorar continuamente o modelo BERT
""")

st.markdown("---")

# Input
st.subheader("üìù Teste de Avalia√ß√£o")

review_text = st.text_area(
    "Review para An√°lise",
    height=100,
    placeholder="Digite um review para que BERT analise e GPT avalie...",
    help="Review que ser√° analisado pelo BERT e depois avaliado pelo GPT"
)

# Op√ß√µes
col1, col2 = st.columns(2)

with col1:
    show_reasoning = st.checkbox("Mostrar racioc√≠nio do GPT", value=True)

with col2:
    show_confidence = st.checkbox("Mostrar n√≠veis de confian√ßa", value=True)

# Bot√£o
col1, col2, col3 = st.columns([1, 2, 1])

with col2:
    judge_button = st.button("‚öñÔ∏è Analisar e Julgar", type="primary", use_container_width=True)

# Processar
if judge_button:
    if not review_text.strip():
        st.error("‚ùå Digite um review para an√°lise!")
    else:
        # Fase 1: BERT prediz
        st.markdown("---")
        st.subheader("üéØ Fase 1: Predi√ß√£o BERT")
        
        with st.spinner("üß† BERT analisando..."):
            time.sleep(0.5)
            
            # Simula√ß√£o de predi√ß√£o BERT
            bert_prediction = "positive"
            bert_confidence = 0.82
            bert_scores = {
                'positive': 0.82,
                'neutral': 0.12,
                'negative': 0.06
            }
            
            col1, col2 = st.columns(2)
            
            with col1:
                sentiment_emoji = {
                    'positive': 'üòä',
                    'neutral': 'üòê',
                    'negative': 'üòû'
                }
                
                sentiment_color = {
                    'positive': '#28a745',
                    'neutral': '#ffc107',
                    'negative': '#dc3545'
                }
                
                st.markdown(f"""
                <div style="text-align: center; padding: 1.5rem; background: {sentiment_color[bert_prediction]}; border-radius: 10px; color: white;">
                    <h1 style="font-size: 3rem; margin: 0;">{sentiment_emoji[bert_prediction]}</h1>
                    <h2>BERT: {bert_prediction.upper()}</h2>
                    <h3>Confian√ßa: {bert_confidence*100:.1f}%</h3>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                if show_confidence:
                    st.markdown("#### Scores BERT")
                    for sent, score in bert_scores.items():
                        st.markdown(f"**{sent.capitalize()}**: {score*100:.1f}%")
                        st.progress(score)
        
        # Fase 2: GPT julga
        st.markdown("---")
        st.subheader("‚öñÔ∏è Fase 2: Julgamento GPT-4o-mini")
        
        with st.spinner("ü§ñ GPT avaliando a predi√ß√£o..."):
            time.sleep(1.5)
            
            # Simula√ß√£o de julgamento GPT
            gpt_verdict = "correct"  # correct, incorrect, ambiguous
            gpt_confidence = 0.88
            gpt_reasoning = """
            **An√°lise do Review:**
            O texto apresenta caracter√≠sticas predominantemente positivas, incluindo 
            termos como "delicioso" e "recomendo". Embora haja uma men√ß√£o neutra sobre 
            o pre√ßo, o tom geral √© de satisfa√ß√£o.
            
            **Avalia√ß√£o da Predi√ß√£o BERT:**
            ‚úÖ A classifica√ß√£o como "Positive" est√° **CORRETA**
            
            **Justificativa:**
            - O sentimento dominante √© claramente positivo
            - A confian√ßa de 82% √© adequada (n√£o h√° ambiguidade significativa)
            - A distribui√ß√£o de scores reflete bem a an√°lise (82% pos, 12% neu, 6% neg)
            
            **Sugest√µes de Melhoria:**
            - BERT poderia ter maior confian√ßa (90%+) dado o tom claramente positivo
            - Aspecto "pre√ßo" poderia ser tratado separadamente em an√°lise multi-aspecto
            """
            
            alternative_prediction = None
            
            # Mostrar veredicto
            col1, col2, col3 = st.columns(3)
            
            with col1:
                if gpt_verdict == "correct":
                    st.success("### ‚úÖ CORRETO")
                    st.metric("Confian√ßa do Juiz", f"{gpt_confidence*100:.0f}%")
                elif gpt_verdict == "incorrect":
                    st.error("### ‚ùå INCORRETO")
                    st.metric("Confian√ßa do Juiz", f"{gpt_confidence*100:.0f}%")
                    st.warning(f"Deveria ser: **{alternative_prediction}**")
                else:
                    st.warning("### ‚ö†Ô∏è AMB√çGUO")
                    st.metric("Confian√ßa do Juiz", f"{gpt_confidence*100:.0f}%")
            
            with col2:
                st.markdown("### Concord√¢ncia")
                st.metric("BERT vs GPT", "‚úÖ Concordam")
            
            with col3:
                st.markdown("### Score Final")
                final_score = 8.5
                st.metric("Nota", f"{final_score}/10")
            
            # Racioc√≠nio detalhado
            if show_reasoning:
                st.markdown("---")
                st.markdown("### üí≠ Racioc√≠nio Detalhado do GPT")
                st.markdown(gpt_reasoning)

# Estat√≠sticas
st.markdown("---")
st.subheader("üìä Estat√≠sticas do LLM Judge")

col1, col2, col3, col4 = st.columns(4)

with col1:
    st.metric("Avalia√ß√µes Totais", "5,432")

with col2:
    st.metric("Taxa de Acerto BERT", "91.3%")

with col3:
    st.metric("Casos Amb√≠guos", "7.2%")

with col4:
    st.metric("Concord√¢ncia", "89.5%")

# Casos recentes
with st.expander("üìã Casos Avaliados Recentemente"):
    import pandas as pd
    
    cases_df = pd.DataFrame({
        'Review': [
            'Comida excelente!',
            'Entrega demorou, mas valeu a pena',
            'N√£o gostei, caro demais',
            'Ok, nada especial'
        ],
        'BERT': ['Positive', 'Positive', 'Negative', 'Neutral'],
        'GPT Verdict': ['‚úÖ Correto', '‚ö†Ô∏è Amb√≠guo', '‚úÖ Correto', '‚úÖ Correto'],
        'Score': ['9/10', '6/10', '8/10', '7/10']
    })
    
    st.dataframe(cases_df, hide_index=True, use_container_width=True)

# Framework de Avalia√ß√£o
st.markdown("---")
st.subheader("üéØ Framework de Avalia√ß√£o")

col1, col2 = st.columns(2)

with col1:
    st.markdown("### Crit√©rios do GPT Judge")
    st.markdown("""
    1. **Acur√°cia**: Predi√ß√£o correta?
    2. **Confian√ßa**: N√≠vel apropriado?
    3. **Nuance**: Capturou sutilezas?
    4. **Contexto**: Considerou contexto completo?
    5. **Aspectos**: Identificou m√∫ltiplos aspectos?
    
    **Scoring:**
    - 9-10: Excelente
    - 7-8: Bom
    - 5-6: Aceit√°vel
    - 3-4: Ruim
    - 0-2: Muito ruim
    """)

with col2:
    st.markdown("### A√ß√µes Baseadas no Feedback")
    st.markdown("""
    **Score Alto (8-10):**
    - ‚úÖ Mant√©m modelo como est√°
    - ‚úÖ Usa caso para valida√ß√£o
    
    **Score M√©dio (5-7):**
    - ‚ö†Ô∏è Adiciona caso ao dataset de treino
    - ‚ö†Ô∏è Investiga padr√£o de erro
    
    **Score Baixo (0-4):**
    - ‚ùå Prioriza corre√ß√£o
    - ‚ùå Re-treina modelo
    - ‚ùå Analisa features faltantes
    """)

# Insights
with st.expander("üí° Insights e Aprendizados"):
    st.markdown("""
    ### Padr√µes Identificados pelo LLM Judge
    
    **BERT tende a errar quando:**
    - Reviews muito longos (> 500 palavras)
    - Sarcasmo ou ironia presentes
    - Sentimentos mistos (positivo + negativo)
    - Contexto cultural espec√≠fico
    
    **BERT √© excelente quando:**
    - Reviews diretos e claros
    - Vocabul√°rio comum de reviews
    - Sentimento uniforme
    - Tamanho m√©dio (50-200 palavras)
    
    ### Melhorias Implementadas
    
    - ‚úÖ Aumentado dataset com casos amb√≠guos
    - ‚úÖ Fine-tuning adicional em sarcasmo
    - ‚úÖ Melhor tratamento de reviews longos
    - üîÑ Em progresso: Multi-aspect analysis
    """)

# Configura√ß√µes
with st.expander("‚öôÔ∏è Configura√ß√µes Avan√ßadas"):
    st.markdown("### Par√¢metros do LLM Judge")
    
    threshold_agreement = st.slider("Threshold de Concord√¢ncia", 0.5, 1.0, 0.75)
    threshold_confidence = st.slider("Threshold de Confian√ßa BERT", 0.5, 1.0, 0.70)
    enable_learning = st.checkbox("Habilitar aprendizado cont√≠nuo", value=True)
    
    st.info("""
    **Thresholds configurados:**
    - Concord√¢ncia < 75%: Caso marcado para revis√£o
    - Confian√ßa BERT < 70%: Solicita avalia√ß√£o GPT
    - Aprendizado cont√≠nuo: Casos avaliados alimentam re-treino
    """)

# Info
st.markdown("---")
st.success("""
üí° **LLM as a Judge** √© uma t√©cnica poderosa para valida√ß√£o e melhoria cont√≠nua de modelos.
GPT-4o-mini atua como "professor" do BERT, identificando casos problem√°ticos e sugerindo melhorias.
""")
