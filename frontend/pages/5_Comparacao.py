"""
SentiBR - Compara√ß√£o BERT vs GPT
Compara predi√ß√µes entre BERT e GPT-4o-mini
"""
import streamlit as st
import pandas as pd
import time

st.set_page_config(page_title="Compara√ß√£o", page_icon="‚öîÔ∏è", layout="wide")

st.title("‚öîÔ∏è Compara√ß√£o: BERT vs GPT-4o-mini")
st.markdown("Compare as predi√ß√µes dos dois modelos lado a lado")

# Informa√ß√µes dos modelos
col1, col2 = st.columns(2)

with col1:
    st.markdown("""
    ### üß† BERT (Fine-tuned)
    - **Modelo**: neuralmind/bert-base-portuguese-cased
    - **Lat√™ncia**: ~100ms
    - **Custo**: Gr√°tis (local)
    - **Acur√°cia**: 92.3%
    """)

with col2:
    st.markdown("""
    ### ü§ñ GPT-4o-mini
    - **Modelo**: OpenAI GPT-4o-mini
    - **Lat√™ncia**: ~2000ms
    - **Custo**: $0.15/1M tokens
    - **Acur√°cia**: ~94% (estimado)
    """)

st.markdown("---")

# Input de review
st.subheader("Digite o Review para Compara√ß√£o")

review_text = st.text_area(
    "Review do Restaurante",
    height=150,
    placeholder="Exemplo: A comida estava deliciosa mas a entrega demorou muito...",
    help="Digite um review para comparar as predi√ß√µes"
)

# Bot√£o de compara√ß√£o
col1, col2, col3 = st.columns([1, 2, 1])

with col2:
    compare_button = st.button("‚öîÔ∏è Comparar Modelos", type="primary", use_container_width=True)

# Processar compara√ß√£o
if compare_button:
    if not review_text.strip():
        st.error("‚ùå Digite um review para compara√ß√£o!")
    else:
        st.markdown("---")
        
        # Criar duas colunas para resultados lado a lado
        col_bert, col_gpt = st.columns(2)
        
        # BERT
        with col_bert:
            st.markdown("### üß† BERT")
            
            with st.spinner("Analisando com BERT..."):
                time.sleep(0.5)  # Simular lat√™ncia
                
                # Simula√ß√£o de resultado BERT
                bert_sentiment = "positive"
                bert_confidence = 0.87
                bert_scores = {
                    'positive': 0.87,
                    'neutral': 0.09,
                    'negative': 0.04
                }
                bert_latency = 95
                
                # Exibir resultado
                sentiment_emoji = {
                    'positive': 'üòä',
                    'neutral': 'üòê',
                    'negative': 'üòû'
                }
                
                sentiment_color = {
                    'positive': '#28a745',
                    'neutral': '#ffc107',
                    'negative': '#dc3545'
                }
                
                st.markdown(f"""
                <div style="text-align: center; padding: 1.5rem; background: {sentiment_color[bert_sentiment]}; border-radius: 10px; color: white; margin: 1rem 0;">
                    <h1 style="font-size: 3rem; margin: 0;">{sentiment_emoji[bert_sentiment]}</h1>
                    <h2>{bert_sentiment.upper()}</h2>
                    <h3>Confian√ßa: {bert_confidence*100:.1f}%</h3>
                </div>
                """, unsafe_allow_html=True)
                
                st.markdown("#### Probabilidades")
                for sent, score in bert_scores.items():
                    st.markdown(f"**{sent.capitalize()}**: {score*100:.1f}%")
                    st.progress(score)
                
                st.metric("‚ö° Lat√™ncia", f"{bert_latency}ms")
        
        # GPT
        with col_gpt:
            st.markdown("### ü§ñ GPT-4o-mini")
            
            with st.spinner("Analisando com GPT..."):
                time.sleep(1.5)  # Simular lat√™ncia maior
                
                # Simula√ß√£o de resultado GPT
                gpt_sentiment = "positive"
                gpt_confidence = 0.92
                gpt_reasoning = """
                O review expressa satisfa√ß√£o com a comida ("deliciosa"), 
                mas tamb√©m menciona um problema com a entrega ("demorou muito"). 
                No entanto, o aspecto positivo (qualidade da comida) parece ter 
                maior peso no sentimento geral, resultando em classifica√ß√£o positiva.
                """
                gpt_latency = 1847
                
                st.markdown(f"""
                <div style="text-align: center; padding: 1.5rem; background: {sentiment_color[gpt_sentiment]}; border-radius: 10px; color: white; margin: 1rem 0;">
                    <h1 style="font-size: 3rem; margin: 0;">{sentiment_emoji[gpt_sentiment]}</h1>
                    <h2>{gpt_sentiment.upper()}</h2>
                    <h3>Confian√ßa: {gpt_confidence*100:.1f}%</h3>
                </div>
                """, unsafe_allow_html=True)
                
                st.markdown("#### Racioc√≠nio")
                st.info(gpt_reasoning)
                
                st.metric("‚ö° Lat√™ncia", f"{gpt_latency}ms")
        
        # Compara√ß√£o final
        st.markdown("---")
        st.subheader("üìä An√°lise Comparativa")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("#### Concord√¢ncia")
            if bert_sentiment == gpt_sentiment:
                st.success("‚úÖ Modelos concordam!")
            else:
                st.warning("‚ö†Ô∏è Modelos divergem")
        
        with col2:
            st.markdown("#### Diferen√ßa de Confian√ßa")
            diff = abs(bert_confidence - gpt_confidence) * 100
            st.metric("Diferen√ßa", f"{diff:.1f}%")
        
        with col3:
            st.markdown("#### Diferen√ßa de Lat√™ncia")
            latency_diff = gpt_latency - bert_latency
            st.metric("Mais lento", f"+{latency_diff}ms", delta_color="inverse")
        
        # Tabela comparativa
        st.markdown("#### Compara√ß√£o Detalhada")
        
        comparison_df = pd.DataFrame({
            'M√©trica': ['Sentimento', 'Confian√ßa', 'Lat√™ncia', 'Custo'],
            'BERT': [
                bert_sentiment,
                f"{bert_confidence*100:.1f}%",
                f"{bert_latency}ms",
                "Gr√°tis"
            ],
            'GPT-4o-mini': [
                gpt_sentiment,
                f"{gpt_confidence*100:.1f}%",
                f"{gpt_latency}ms",
                "~$0.0001"
            ]
        })
        
        st.dataframe(comparison_df, hide_index=True, use_container_width=True)

# Trade-offs
st.markdown("---")
st.subheader("‚öñÔ∏è Trade-offs entre os Modelos")

col1, col2 = st.columns(2)

with col1:
    st.markdown("### üß† BERT - Vantagens")
    st.markdown("""
    ‚úÖ **Lat√™ncia baixa** (~100ms)  
    ‚úÖ **Custo zero** (roda local)  
    ‚úÖ **Previs√≠vel** (comportamento consistente)  
    ‚úÖ **Escal√°vel** (milhares de req/s)  
    ‚úÖ **Offline** (n√£o precisa internet)  
    
    ‚ùå **Limita√ß√µes:**
    - Menos contextual que GPT
    - Sem racioc√≠nio expl√≠cito
    - Precisa fine-tuning
    """)

with col2:
    st.markdown("### ü§ñ GPT - Vantagens")
    st.markdown("""
    ‚úÖ **Maior acur√°cia** (~94%)  
    ‚úÖ **Racioc√≠nio** (explica decis√µes)  
    ‚úÖ **Contextual** (entende nuances)  
    ‚úÖ **Zero-shot** (sem fine-tuning)  
    ‚úÖ **Flexible** (m√∫ltiplas tarefas)  
    
    ‚ùå **Limita√ß√µes:**
    - Alta lat√™ncia (~2s)
    - Custo por requisi√ß√£o
    - Precisa internet
    - Rate limits
    """)

# Casos de uso
st.markdown("---")
st.subheader("üéØ Quando Usar Cada Modelo")

col1, col2 = st.columns(2)

with col1:
    st.markdown("### Use BERT para:")
    st.info("""
    - **Alto volume** (milhares de reviews/dia)
    - **Tempo real** (an√°lise instant√¢nea)
    - **Produ√ß√£o** (baixo custo, alta disponibilidade)
    - **Batch processing** (milh√µes de reviews)
    - **Edge cases** (offline, lat√™ncia cr√≠tica)
    """)

with col2:
    st.markdown("### Use GPT para:")
    st.info("""
    - **An√°lise profunda** (reviews complexos)
    - **Explicabilidade** (precisa justificar)
    - **Casos amb√≠guos** (sentimento misto)
    - **Baixo volume** (poucos reviews cr√≠ticos)
    - **Prototipagem** (teste r√°pido sem treino)
    """)

# Estat√≠sticas
with st.expander("üìà Estat√≠sticas de Compara√ß√£o"):
    st.markdown("""
    ### Dados Coletados (√∫ltimos 1000 reviews)
    
    - **Taxa de concord√¢ncia**: 87.3%
    - **Casos onde GPT foi melhor**: 9.2%
    - **Casos onde BERT foi melhor**: 3.5%
    - **Custo m√©dio GPT**: $0.12 por 1000 reviews
    - **Throughput BERT**: 500 req/s
    - **Throughput GPT**: 30 req/s
    """)

# Recomenda√ß√£o
st.markdown("---")
st.info("""
üí° **Recomenda√ß√£o**: Use **BERT para produ√ß√£o** (volume alto, baixo custo, lat√™ncia baixa) 
e **GPT para casos especiais** (an√°lise profunda, explicabilidade, ambiguidade).

Estrat√©gia h√≠brida: BERT como modelo principal + GPT para valida√ß√£o de casos incertos (confian√ßa < 70%).
""")
