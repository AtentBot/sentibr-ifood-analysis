# üöÄ DEPLOYMENT GUIDE - Fase 6

Guia completo para integrar a Fase 6 ao seu projeto SentiBR.

## üìã Pr√©-requisitos

Antes de come√ßar, certifique-se que voc√™ tem:

‚úÖ Python 3.10+  
‚úÖ Projeto SentiBR configurado (Fases 1-5)  
‚úÖ OpenAI API key  
‚úÖ Modelo BERT treinado em `models/bert_finetuned/`  
‚úÖ Dados de teste em `data/processed/test.csv`  

---

## üîß Passo 1: Estrutura de Diret√≥rios

### 1.1 Verificar Estrutura Atual

```bash
cd seu-projeto-sentibr
tree -L 2
```

Voc√™ deve ter algo como:
```
sentibr/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ models/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ training/
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îî‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ scripts/
‚îú‚îÄ‚îÄ logs/
‚îî‚îÄ‚îÄ requirements.txt
```

### 1.2 Criar Estrutura para Fase 6

```bash
# Criar diret√≥rios necess√°rios
mkdir -p src/evaluation
mkdir -p logs/evaluation
mkdir -p logs/llm_judge
```

---

## üì¶ Passo 2: Copiar Arquivos

### 2.1 M√≥dulo de Evaluation

```bash
# Copiar m√≥dulo completo
cp -r fase6_eval_llm/evaluation/* src/evaluation/

# Verificar
ls -lh src/evaluation/
# Deve mostrar: __init__.py, eval_suite.py, llm_judge.py, README.md
```

### 2.2 Script de Execu√ß√£o

```bash
# Copiar script
cp fase6_eval_llm/run_evaluation.py scripts/

# Dar permiss√£o de execu√ß√£o
chmod +x scripts/run_evaluation.py

# Verificar
ls -lh scripts/run_evaluation.py
```

### 2.3 Configura√ß√µes

```bash
# Copiar requirements
cat fase6_eval_llm/requirements-evaluation.txt >> requirements.txt

# Copiar .env example (se n√£o tiver)
if [ ! -f .env ]; then
    cp fase6_eval_llm/.env.example .env
    echo "‚ö†Ô∏è  ATEN√á√ÉO: Edite .env e adicione sua OPENAI_API_KEY"
fi
```

---

## üîê Passo 3: Configurar Vari√°veis de Ambiente

### 3.1 Editar .env

```bash
nano .env
```

Adicione:
```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-your-actual-key-here
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.1
LLM_MAX_TOKENS=1000

# Evaluation Configuration
EVAL_OUTPUT_DIR=logs/evaluation
EVAL_MAX_SAMPLES=1000
LLM_JUDGE_SAMPLES=100
```

### 3.2 Verificar Configura√ß√£o

```bash
# Carregar vari√°veis
source .env

# Testar
echo $OPENAI_API_KEY
# Deve mostrar sua API key
```

---

## üìö Passo 4: Instalar Depend√™ncias

### 4.1 Instalar Novas Depend√™ncias

```bash
# Instalar requirements
pip install -r requirements.txt

# Ou instalar apenas as novas
pip install openai>=1.0.0 matplotlib>=3.7.0 seaborn>=0.12.0
```

### 4.2 Verificar Instala√ß√£o

```bash
# Testar imports
python -c "from src.evaluation import ModelEvaluator, LLMJudge; print('‚úÖ Imports OK')"

# Testar OpenAI
python -c "import openai; print('‚úÖ OpenAI OK')"
```

---

## ‚úÖ Passo 5: Validar Integra√ß√£o

### 5.1 Teste R√°pido - Evaluation Suite

```bash
# Testar eval_suite standalone
python src/evaluation/eval_suite.py
```

**Esperado:** Ver exemplo de avalia√ß√£o com m√©tricas.

### 5.2 Teste R√°pido - LLM Judge

```bash
# Testar llm_judge standalone (requer API key)
export OPENAI_API_KEY='your-key'
python src/evaluation/llm_judge.py
```

**Esperado:** Ver 4 casos de teste sendo julgados pelo GPT-4o-mini.

### 5.3 Teste de Integra√ß√£o

```bash
# Executar avalia√ß√£o em 10 samples (sem LLM, para teste r√°pido)
python scripts/run_evaluation.py --samples 10

# Ver resultados
ls -lh logs/evaluation/
cat logs/evaluation/bert_report_*.txt
```

**Esperado:** 
- ‚úÖ Script executa sem erros
- ‚úÖ M√©tricas calculadas
- ‚úÖ Confusion matrix gerada
- ‚úÖ Relat√≥rio criado

### 5.4 Teste com LLM (Cuidado: Custa ~$0.001)

```bash
# Executar com LLM Judge em 5 samples
python scripts/run_evaluation.py --samples 10 --use-llm --llm-samples 5

# Ver resultados LLM
cat logs/evaluation/final_report_*.txt
```

**Esperado:**
- ‚úÖ LLM Judge executa
- ‚úÖ Julgamentos salvos em JSON
- ‚úÖ M√©tricas de agreement calculadas
- ‚úÖ Relat√≥rio final gerado

---

## üîó Passo 6: Integrar com API Existente

### 6.1 Adicionar Endpoint de Avalia√ß√£o (Opcional)

Edite `src/api/main.py` e adicione:

```python
from src.evaluation import ModelEvaluator, LLMJudge

@app.post("/api/v1/evaluate", tags=["Evaluation"])
async def evaluate_model(
    test_samples: int = 100,
    use_llm: bool = False
):
    """
    Executa avalia√ß√£o do modelo em test set
    
    **Diferencial:** Permite avalia√ß√£o on-demand via API
    """
    try:
        # Carregar test data
        test_df = pd.read_csv("data/processed/test.csv").head(test_samples)
        
        # Fazer predi√ß√µes
        predictor = get_predictor()
        predictions = []
        for text in test_df['text']:
            result = predictor.predict(text)
            predictions.append(result['sentiment'])
        
        # Avaliar com BERT
        evaluator = ModelEvaluator(model_name="BERT Fine-tuned")
        result = evaluator.evaluate(
            y_true=test_df['label'].values,
            y_pred=np.array(predictions)
        )
        
        response = {
            "model": "BERT Fine-tuned",
            "samples_evaluated": test_samples,
            "accuracy": result.accuracy,
            "f1_score": result.f1_score,
            "precision": result.precision,
            "recall": result.recall
        }
        
        # Avaliar com LLM se solicitado
        if use_llm and os.getenv("OPENAI_API_KEY"):
            judge = LLMJudge()
            llm_results, metrics = judge.judge_batch(
                texts=test_df['text'].tolist()[:10],
                bert_preds=predictions[:10],
                max_samples=10
            )
            response["llm_agreement"] = metrics['bert_agreement_rate']
        
        return response
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 6.2 Testar Novo Endpoint

```bash
# Iniciar API
uvicorn src.api.main:app --reload

# Testar endpoint
curl -X POST "http://localhost:8000/api/v1/evaluate?test_samples=50&use_llm=false"
```

---

## üìä Passo 7: Adicionar ao Frontend (Opcional)

### 7.1 Criar P√°gina de Avalia√ß√£o

Crie `frontend/pages/4_üìä_Avalia√ß√£o.py`:

```python
import streamlit as st
import requests
import pandas as pd

st.title("üìä Avalia√ß√£o do Modelo")

# Bot√£o para executar avalia√ß√£o
if st.button("üöÄ Executar Avalia√ß√£o"):
    with st.spinner("Executando avalia√ß√£o..."):
        response = requests.post(
            "http://localhost:8000/api/v1/evaluate",
            params={"test_samples": 100, "use_llm": False}
        )
        
        if response.status_code == 200:
            data = response.json()
            
            # Mostrar m√©tricas
            col1, col2, col3, col4 = st.columns(4)
            col1.metric("Accuracy", f"{data['accuracy']:.3f}")
            col2.metric("Precision", f"{data['precision']:.3f}")
            col3.metric("Recall", f"{data['recall']:.3f}")
            col4.metric("F1-Score", f"{data['f1_score']:.3f}")
```

---

## üß™ Passo 8: Criar Testes

### 8.1 Testes Unit√°rios

Crie `tests/unit/test_eval_suite.py`:

```python
import pytest
import numpy as np
from src.evaluation import ModelEvaluator

def test_evaluator_creation():
    evaluator = ModelEvaluator(model_name="Test")
    assert evaluator.model_name == "Test"

def test_evaluate_perfect_predictions():
    evaluator = ModelEvaluator(model_name="Test")
    y_true = np.array([0, 1, 2])
    y_pred = np.array([0, 1, 2])
    
    result = evaluator.evaluate(y_true, y_pred)
    
    assert result.accuracy == 1.0
    assert result.f1_score == 1.0
```

### 8.2 Executar Testes

```bash
pytest tests/unit/test_eval_suite.py -v
```

---

## üìù Passo 9: Atualizar Documenta√ß√£o

### 9.1 Atualizar README Principal

Adicione ao `README.md`:

```markdown
## üìä Fase 6: Avalia√ß√£o e LLM Integration

Sistema completo de avalia√ß√£o com:
- ‚úÖ M√©tricas cl√°ssicas (Accuracy, F1, etc)
- ‚úÖ LLM-as-Judge com GPT-4o-mini
- ‚úÖ Compara√ß√£o BERT vs GPT
- ‚úÖ An√°lise de edge cases

### Executar Avalia√ß√£o

\```bash
python scripts/run_evaluation.py --samples 100 --use-llm
\```

### Documenta√ß√£o

Ver [src/evaluation/README.md](src/evaluation/README.md)
```

### 9.2 Atualizar Changelog

Adicione ao `CHANGELOG.md`:

```markdown
## [1.6.0] - 2024-11-06

### Added
- ‚ú® Evaluation Suite completo
- ‚ú® LLM-as-Judge com GPT-4o-mini
- ‚ú® Compara√ß√£o BERT vs GPT
- ‚ú® Script de avalia√ß√£o end-to-end
- üìö Documenta√ß√£o extensiva

### Features
- M√©tricas cl√°ssicas (Accuracy, Precision, Recall, F1)
- Confusion Matrix e visualiza√ß√µes
- An√°lise detalhada de erros
- Identifica√ß√£o de edge cases
- An√°lise por aspectos
- Tracking de custos
```

---

## üéØ Passo 10: Executar Avalia√ß√£o Completa

### 10.1 Avalia√ß√£o de Produ√ß√£o

```bash
# Avalia√ß√£o completa no test set
python scripts/run_evaluation.py \
    --test-file data/processed/test.csv \
    --samples 1000 \
    --use-llm \
    --llm-samples 100 \
    --output-dir logs/evaluation/production

# Estimar custo: ~$0.015
```

### 10.2 Verificar Resultados

```bash
# Ver todos os outputs
ls -lh logs/evaluation/production/

# Ler relat√≥rio final
cat logs/evaluation/production/final_report_*.txt

# Verificar m√©tricas BERT
cat logs/evaluation/production/bert_evaluation_*.json | jq '.accuracy, .f1_score'

# Verificar m√©tricas LLM
cat logs/evaluation/production/metrics_*.json | jq '.bert_agreement_rate'
```

---

## ‚úÖ Checklist de Deployment

Use este checklist para garantir que tudo est√° funcionando:

### Estrutura
- [ ] `src/evaluation/` criado com todos os arquivos
- [ ] `scripts/run_evaluation.py` copiado
- [ ] `logs/evaluation/` e `logs/llm_judge/` criados

### Configura√ß√£o
- [ ] Depend√™ncias instaladas (`pip install -r requirements.txt`)
- [ ] `.env` configurado com `OPENAI_API_KEY`
- [ ] Vari√°veis de ambiente carregadas

### Testes B√°sicos
- [ ] `python src/evaluation/eval_suite.py` executa sem erros
- [ ] `python src/evaluation/llm_judge.py` executa com API key
- [ ] `python scripts/run_evaluation.py --samples 10` funciona

### Integra√ß√£o
- [ ] Imports funcionam: `from src.evaluation import ModelEvaluator`
- [ ] API predictor acess√≠vel
- [ ] Test data dispon√≠vel em `data/processed/test.csv`

### Avalia√ß√£o Completa
- [ ] Avalia√ß√£o BERT completa executada
- [ ] LLM Judge testado (pelo menos 10 samples)
- [ ] Relat√≥rios gerados corretamente
- [ ] Visualiza√ß√µes criadas (confusion matrix)

### Documenta√ß√£o
- [ ] README principal atualizado
- [ ] CHANGELOG atualizado
- [ ] Documenta√ß√£o da Fase 6 lida

### Valida√ß√£o Final
- [ ] Accuracy > 0.90 ‚úÖ
- [ ] F1-Score > 0.88 ‚úÖ
- [ ] LLM Agreement > 0.85 ‚úÖ
- [ ] Custos sob controle ‚úÖ

---

## üêõ Troubleshooting Comum

### Problema 1: Import Error

**Erro:**
```
ImportError: cannot import name 'ModelEvaluator' from 'src.evaluation'
```

**Solu√ß√£o:**
```bash
# Verificar estrutura
ls -lh src/evaluation/__init__.py

# Verificar PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### Problema 2: OpenAI API Key

**Erro:**
```
ValueError: OpenAI API key n√£o encontrada
```

**Solu√ß√£o:**
```bash
# Exportar temporariamente
export OPENAI_API_KEY='your-key'

# Ou adicionar ao .env
echo "OPENAI_API_KEY=your-key" >> .env
source .env
```

### Problema 3: Model Not Found

**Erro:**
```
FileNotFoundError: models/bert_finetuned not found
```

**Solu√ß√£o:**
```bash
# Verificar se modelo existe
ls -lh models/bert_finetuned/

# Se n√£o existe, treinar
python src/training/train.py
```

### Problema 4: Test Data Missing

**Erro:**
```
FileNotFoundError: data/processed/test.csv not found
```

**Solu√ß√£o:**
```bash
# Preparar dados
python src/data/load_data_v2.py
python src/data/split_dataset.py
```

---

## üöÄ Pr√≥ximos Passos

Ap√≥s deployment bem-sucedido:

### Imediato
1. ‚úÖ Executar avalia√ß√£o completa no test set
2. ‚úÖ Documentar m√©tricas obtidas
3. ‚úÖ Integrar com CI/CD (se aplic√°vel)

### Fase 7: Docker
1. Containerizar aplica√ß√£o completa
2. Docker Compose com todos os servi√ßos
3. Deploy em produ√ß√£o

### Fase 8: Testes
1. Unit tests completos
2. Integration tests
3. Load tests com Locust

---

## üí° Dicas de Uso em Produ√ß√£o

### 1. Continuous Evaluation

Crie um cron job para avaliar periodicamente:

```bash
# Adicionar ao crontab
0 0 * * 0 cd /path/to/sentibr && python scripts/run_evaluation.py --samples 500 --use-llm --llm-samples 50
```

### 2. Alert em Quedas de Performance

```python
# Em scripts/run_evaluation.py
if result.accuracy < 0.90:
    send_alert(f"Accuracy dropped to {result.accuracy}")
```

### 3. Cost Control

```python
# Limitar gastos LLM
MAX_COST = 1.00  # USD
if judge.total_cost > MAX_COST:
    raise ValueError(f"Cost limit exceeded: ${judge.total_cost}")
```

---

## üìä M√©tricas de Sucesso

Ap√≥s deployment, voc√™ deve ter:

- ‚úÖ Accuracy > 90% no test set
- ‚úÖ F1-Score > 88%
- ‚úÖ LLM Agreement > 85%
- ‚úÖ Tempo de avalia√ß√£o < 10min para 1000 samples
- ‚úÖ Custo LLM < $0.20 para avalia√ß√£o completa

---

## üéâ Conclus√£o

Parab√©ns! Voc√™ integrou com sucesso a **Fase 6: EVAL E LLM INTEGRATION** ao seu projeto SentiBR.

Agora voc√™ tem:
- ‚úÖ Sistema completo de avalia√ß√£o
- ‚úÖ LLM-as-Judge funcional
- ‚úÖ Compara√ß√£o BERT vs GPT
- ‚úÖ An√°lise de edge cases
- ‚úÖ Tracking de custos
- ‚úÖ Documenta√ß√£o completa

**Pr√≥ximo passo:** [Fase 7 - Docker + Deploy](../docker/README.md)

---

**Desenvolvido com ‚ù§Ô∏è para o desafio t√©cnico de IA S√™nior**
