# üöÄ Quick Start - Fase 6: EVAL E LLM INTEGRATION

Guia r√°pido para come√ßar a usar o sistema de avalia√ß√£o e LLM integration do SentiBR.

## ‚ö° Setup em 5 Minutos

### 1. Instalar Depend√™ncias

```bash
# Instalar depend√™ncias da Fase 6
pip install -r requirements-evaluation.txt
```

### 2. Configurar OpenAI API

```bash
# Copiar exemplo de configura√ß√£o
cp .env.example .env

# Editar e adicionar sua API key
nano .env

# Ou exportar diretamente
export OPENAI_API_KEY='your-api-key-here'
```

### 3. Executar Avalia√ß√£o B√°sica

```bash
# Avalia√ß√£o BERT apenas (sem LLM)
python scripts/run_evaluation.py --samples 100

# Com LLM Judge (100 samples)
python scripts/run_evaluation.py --samples 1000 --use-llm --llm-samples 100
```

### 4. Ver Resultados

```bash
# Resultados salvos em:
ls -lh logs/evaluation/

# Visualizar relat√≥rio
cat logs/evaluation/final_report_*.txt
```

---

## üìä Exemplos de Uso

### Exemplo 1: Avalia√ß√£o R√°pida BERT

```python
from src.evaluation import ModelEvaluator
import numpy as np

# Dados de exemplo
y_true = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])
y_pred = np.array([0, 1, 2, 1, 1, 1, 0, 2, 2])

# Avaliar
evaluator = ModelEvaluator(model_name="BERT Fine-tuned")
result = evaluator.evaluate(y_true, y_pred)

# Ver resultado
print(result.summary())
```

**Sa√≠da:**
```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë              EVALUATION SUMMARY - BERT Fine-tuned              
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üìä Overall Metrics:
   ‚Ä¢ Accuracy:  0.6667
   ‚Ä¢ Precision: 0.6667
   ‚Ä¢ Recall:    0.6667
   ‚Ä¢ F1-Score:  0.6667
...
```

### Exemplo 2: LLM Judge em Um Sample

```python
from src.evaluation import LLMJudge

# Criar judge
judge = LLMJudge(model="gpt-4o-mini")

# Julgar uma predi√ß√£o
result = judge.judge_single(
    text="A pizza estava divina! Chegou quentinha e super r√°pido.",
    bert_pred="positivo"
)

# Ver resultado
print(f"LLM Judgment: {result.llm_judgment}")
print(f"Confidence: {result.confidence}")
print(f"Agrees with BERT: {result.agreement_with_bert}")
print(f"\nExplanation: {result.explanation}")
```

**Sa√≠da:**
```
LLM Judgment: positivo
Confidence: 0.95
Agrees with BERT: True

Explanation: O review expressa clara satisfa√ß√£o com a pizza...
```

### Exemplo 3: Batch Evaluation

```python
import pandas as pd
from src.evaluation import ModelEvaluator, LLMJudge

# Carregar dados
test_df = pd.read_csv("data/processed/test.csv").head(100)

# 1. Avaliar com BERT
evaluator = ModelEvaluator(model_name="BERT")
# ... fazer predi√ß√µes ...
bert_result = evaluator.evaluate(y_true, y_pred)

# 2. Avaliar com LLM
judge = LLMJudge()
llm_results, metrics = judge.judge_batch(
    texts=test_df['text'].tolist(),
    bert_preds=predictions_list,
    max_samples=50  # Avaliar apenas 50 para economizar
)

# 3. Ver m√©tricas
judge.print_summary(metrics)
```

### Exemplo 4: Compara√ß√£o Visual

```python
# Gerar confusion matrix
evaluator.plot_confusion_matrix(y_true, y_pred)

# Comparar m√∫ltiplos modelos
bert_result = evaluator_bert.evaluate(y_true, y_pred_bert)
gpt_result = evaluator_gpt.evaluate(y_true, y_pred_gpt)

evaluator.plot_metrics_comparison([bert_result, gpt_result])
```

---

## üéØ Casos de Uso Comuns

### Caso 1: Avaliar Novo Modelo

Voc√™ treinou um novo modelo e quer avaliar:

```bash
# Fazer predi√ß√µes e salvar
python scripts/run_predictions.py --model models/new_model --output predictions.csv

# Avaliar
python scripts/run_evaluation.py --predictions predictions.csv
```

### Caso 2: Encontrar Casos Dif√≠ceis

Identificar onde o modelo erra mais:

```python
from src.evaluation import LLMJudge

judge = LLMJudge()
results, _ = judge.judge_batch(texts, bert_preds, max_samples=100)

# Filtrar edge cases
edge_cases = [r for r in results if r.is_edge_case]

print(f"Encontrados {len(edge_cases)} casos dif√≠ceis:")
for case in edge_cases[:5]:
    print(f"\nText: {case.text}")
    print(f"BERT: {case.bert_prediction}")
    print(f"LLM: {case.llm_judgment}")
    print(f"Why: {case.explanation[:100]}...")
```

### Caso 3: Validar Antes de Deploy

Antes de fazer deploy em produ√ß√£o:

```bash
# Avalia√ß√£o completa no test set
python scripts/run_evaluation.py \
    --test-file data/processed/test.csv \
    --use-llm \
    --llm-samples 200

# Verificar se m√©tricas atendem threshold
# Accuracy > 0.90 ‚úÖ
# F1-Score > 0.88 ‚úÖ
# LLM Agreement > 0.85 ‚úÖ
```

---

## üí∞ Controle de Custos

### Estimativas de Custo (GPT-4o-mini)

| Samples | Tokens | Custo USD |
|---------|--------|-----------|
| 10 | ~3k | $0.001 |
| 50 | ~15k | $0.005 |
| 100 | ~30k | $0.015 |
| 500 | ~150k | $0.075 |
| 1000 | ~300k | $0.150 |

### Dicas para Economizar

1. **Use sampling inteligente**
   ```python
   # Avaliar apenas casos com baixa confian√ßa
   low_confidence = [i for i, conf in enumerate(confidences) if conf < 0.7]
   judge.judge_batch(texts[low_confidence], preds[low_confidence])
   ```

2. **Cache resultados**
   ```python
   # LLM Judge salva automaticamente em JSON
   # Reutilize se precisar reprocessar
   ```

3. **Use batch processing**
   ```python
   # Mais eficiente que m√∫ltiplas chamadas individuais
   judge.judge_batch(texts, preds, max_samples=100)
   ```

---

## üêõ Troubleshooting

### Problema: "OPENAI_API_KEY not found"

**Solu√ß√£o:**
```bash
export OPENAI_API_KEY='your-key'
# ou
echo "OPENAI_API_KEY=your-key" >> .env
```

### Problema: "Model not found"

**Solu√ß√£o:**
```bash
# Verificar se modelo existe
ls -lh models/bert_finetuned/

# Se n√£o existe, treinar primeiro
python src/training/train.py
```

### Problema: "Rate limit exceeded"

**Solu√ß√£o:**
```python
# Adicionar sleep entre chamadas
import time
for text in texts:
    result = judge.judge_single(text, pred)
    time.sleep(1)  # Esperar 1s entre chamadas
```

### Problema: "Out of memory"

**Solu√ß√£o:**
```bash
# Reduzir n√∫mero de samples
python scripts/run_evaluation.py --samples 500

# Ou processar em batches menores
```

---

## üìö Pr√≥ximos Passos

### Fase 7: Docker + Deploy
- Containerizar aplica√ß√£o completa
- Docker Compose com todos os servi√ßos
- Deploy em nuvem (GCP/AWS/Azure)

### Fase 8: Testes
- Unit tests com pytest
- Integration tests
- Load tests com Locust

### Fase 9: Documenta√ß√£o
- README √©pico
- Documenta√ß√£o de API
- Guias de uso

---

## üéì Recursos Adicionais

### Documenta√ß√£o
- [README da Evaluation](src/evaluation/README.md)
- [OpenAI API Docs](https://platform.openai.com/docs)
- [Scikit-learn Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)

### Exemplos
- [eval_suite.py](src/evaluation/eval_suite.py) - Ver fun√ß√£o `main()`
- [llm_judge.py](src/evaluation/llm_judge.py) - Ver fun√ß√£o `main()`

### Papers
- [Constitutional AI](https://arxiv.org/abs/2212.08073)
- [Judging LLM-as-a-Judge](https://arxiv.org/abs/2306.05685)

---

## ‚úÖ Checklist de Valida√ß√£o

Antes de considerar a Fase 6 completa, verifique:

- [ ] Evaluation Suite funcionando
- [ ] M√©tricas calculadas corretamente
- [ ] Confusion matrix gerada
- [ ] LLM Judge configurado
- [ ] OpenAI API funcionando
- [ ] Batch processing testado
- [ ] Custos sob controle
- [ ] Resultados salvos em JSON
- [ ] Relat√≥rios gerados
- [ ] Documenta√ß√£o lida

---

## üéâ Conclus√£o

Parab√©ns! Voc√™ tem agora um sistema completo de avalia√ß√£o com:

‚úÖ **M√©tricas cl√°ssicas** (Accuracy, F1, etc)  
‚úÖ **LLM-as-Judge** para valida√ß√£o qualitativa  
‚úÖ **Compara√ß√£o BERT vs GPT**  
‚úÖ **An√°lise de edge cases**  
‚úÖ **Visualiza√ß√µes** profissionais  
‚úÖ **Tracking de custos**  

**A Fase 6 est√° completa!** üöÄ

Agora voc√™ pode:
- Avaliar qualquer modelo novo
- Validar predi√ß√µes antes de deploy
- Identificar casos dif√≠ceis
- Comparar diferentes abordagens
- Monitorar qualidade ao longo do tempo

**Pr√≥ximo passo:** [Fase 7 - Docker + Deploy](../docker/README.md)

---

**Desenvolvido com ‚ù§Ô∏è para o desafio t√©cnico de IA S√™nior**
