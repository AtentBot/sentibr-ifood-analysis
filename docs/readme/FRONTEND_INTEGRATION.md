# ğŸ¨ INTEGRAÃ‡ÃƒO FRONTEND - PÃ¡gina de AvaliaÃ§Ã£o

Guia completo para adicionar a pÃ¡gina de avaliaÃ§Ã£o ao frontend Streamlit.

## ğŸ“¦ Arquivos IncluÃ­dos

1. **`4_ğŸ§ª_AvaliaÃ§Ã£o.py`** (450 linhas)
   - PÃ¡gina completa de avaliaÃ§Ã£o
   - Interface interativa
   - VisualizaÃ§Ãµes profissionais

2. **`README_AVALIACAO.md`** (400 linhas)
   - DocumentaÃ§Ã£o completa
   - Guia de uso
   - Troubleshooting

---

## ğŸš€ IntegraÃ§Ã£o em 3 Passos

### Passo 1: Copiar Arquivo

```bash
# Copiar pÃ¡gina para frontend
cp 4_ğŸ§ª_AvaliaÃ§Ã£o.py seu-projeto/frontend/pages/

# Verificar
ls -lh seu-projeto/frontend/pages/4_ğŸ§ª_AvaliaÃ§Ã£o.py
```

### Passo 2: Instalar DependÃªncias

```bash
# JÃ¡ deve estar instalado se seguiu a Fase 6
pip install -r requirements-evaluation.txt

# Verificar
python -c "from src.evaluation import ModelEvaluator; print('OK')"
```

### Passo 3: Configurar (Opcional - para LLM)

```bash
# Para usar LLM Judge
export OPENAI_API_KEY='your-api-key-here'
```

**Pronto!** A pÃ¡gina jÃ¡ aparecerÃ¡ no menu do Streamlit.

---

## ğŸ¯ Como Funciona

### Fluxo de Uso

```
1. UsuÃ¡rio acessa "ğŸ§ª AvaliaÃ§Ã£o"
   â†“
2. Configura parÃ¢metros:
   - NÃºmero de samples (10-1000)
   - Usar LLM Judge? (Sim/NÃ£o)
   - Samples para LLM (5-100)
   â†“
3. Clica em "Executar"
   â†“
4. Sistema executa:
   - Carrega test data
   - Faz prediÃ§Ãµes BERT
   - Calcula mÃ©tricas
   - Executa LLM Judge (se ativado)
   - Gera visualizaÃ§Ãµes
   â†“
5. Exibe resultados:
   - MÃ©tricas BERT
   - Confusion Matrix
   - Resultados LLM
   - ComparaÃ§Ã£o
   â†“
6. Permite download:
   - RelatÃ³rios JSON
   - AnÃ¡lises completas
```

### Arquitetura

```
Frontend (Streamlit)
    â†“
src/evaluation/
    â”œâ”€â”€ ModelEvaluator â†’ MÃ©tricas BERT
    â””â”€â”€ LLMJudge â†’ AvaliaÃ§Ã£o GPT
    â†“
src/api/inference.py
    â””â”€â”€ SentimentPredictor â†’ PrediÃ§Ãµes
    â†“
models/bert_finetuned/
    â””â”€â”€ Modelo treinado
```

---

## ğŸ“Š Interface Detalhada

### SeÃ§Ã£o 1: ConfiguraÃ§Ã£o

```python
# Controles interativos
n_samples = st.slider(...)        # 10-1000
use_llm = st.checkbox(...)        # Sim/NÃ£o
llm_samples = st.slider(...)      # 5-100

# Custo estimado automÃ¡tico
estimated_cost = calcular_custo(llm_samples)
st.info(f"ğŸ’° Custo: ${estimated_cost}")
```

### SeÃ§Ã£o 2: ExecuÃ§Ã£o

```python
if st.button("Executar"):
    # Progress bar em tempo real
    progress = st.progress(0)
    
    # AvaliaÃ§Ã£o BERT
    bert_result = run_bert_evaluation(...)
    
    # AvaliaÃ§Ã£o LLM (opcional)
    if use_llm:
        llm_result = run_llm_evaluation(...)
```

### SeÃ§Ã£o 3: Resultados

```python
# Cards de mÃ©tricas
col1, col2, col3, col4 = st.columns(4)
col1.metric("Accuracy", f"{accuracy:.1%}")
col2.metric("Precision", f"{precision:.1%}")
...

# Confusion Matrix interativa (Plotly)
fig = go.Figure(data=go.Heatmap(...))
st.plotly_chart(fig)

# Tabelas de dados
st.dataframe(df_metrics)
```

### SeÃ§Ã£o 4: Download

```python
# BotÃµes de download
st.download_button(
    label="Download BERT (JSON)",
    data=bert_json,
    file_name="bert_eval.json"
)
```

---

## ğŸ¨ CustomizaÃ§Ã£o

### Mudar Cores

```python
# Em 4_ğŸ§ª_AvaliaÃ§Ã£o.py, seÃ§Ã£o CSS
st.markdown("""
<style>
    .metric-value {
        color: #EA1D2C;  /* Sua cor */
    }
</style>
""")
```

### Adicionar MÃ©tricas

```python
# ApÃ³s display_bert_results()
def display_custom_metric():
    st.metric("Sua MÃ©trica", valor)

display_custom_metric()
```

### Mudar Thresholds

```python
# ValidaÃ§Ã£o customizada
if bert_result.accuracy >= 0.95:  # Seu threshold
    st.success("Excelente!")
```

---

## ğŸ”§ ConfiguraÃ§Ãµes AvanÃ§adas

### Mudar Caminho dos Dados

```python
# Em load_test_data()
test_paths = [
    Path("seu/caminho/test.csv"),
    ...
]
```

### Adicionar Mais Samples

```python
# No slider
n_samples = st.slider(
    ...,
    max_value=5000,  # Aumentar limite
    ...
)
```

### Customizar LLM Model

```python
# Em run_llm_evaluation()
judge = LLMJudge(
    model="gpt-4",  # Usar GPT-4 em vez de mini
    temperature=0.0  # Mais determinÃ­stico
)
```

---

## ğŸ“± Responsividade

A pÃ¡gina Ã© **totalmente responsiva**:

- âœ… Desktop (1920x1080)
- âœ… Laptop (1366x768)
- âœ… Tablet (1024x768)
- âš ï¸ Mobile (limitado - Streamlit nÃ£o Ã© ideal para mobile)

---

## ğŸ¯ Features Implementadas

### âœ… BÃ¡sicas
- [x] ConfiguraÃ§Ã£o de parÃ¢metros
- [x] ExecuÃ§Ã£o de avaliaÃ§Ã£o BERT
- [x] ExibiÃ§Ã£o de mÃ©tricas
- [x] Confusion Matrix
- [x] Download de relatÃ³rios

### âœ… AvanÃ§adas
- [x] LLM Judge integration
- [x] Progress bars em tempo real
- [x] Estimativa de custos
- [x] ValidaÃ§Ã£o automÃ¡tica
- [x] ComparaÃ§Ã£o BERT vs GPT
- [x] AnÃ¡lise de erros
- [x] VisualizaÃ§Ãµes interativas

### âœ… UX
- [x] Cards visuais de mÃ©tricas
- [x] Cores do iFood
- [x] Mensagens de validaÃ§Ã£o
- [x] Loading states
- [x] Error handling
- [x] Tooltips informativos

---

## ğŸ§ª Testing

### Teste Manual

```bash
# 1. Iniciar Streamlit
cd frontend
streamlit run app.py

# 2. Acessar http://localhost:8501

# 3. Clicar em "ğŸ§ª AvaliaÃ§Ã£o"

# 4. Configurar:
#    - 10 samples
#    - Sem LLM

# 5. Clicar em "Executar"

# 6. Verificar:
#    - MÃ©tricas aparecem
#    - Confusion matrix renderiza
#    - Download funciona
```

### Teste com LLM

```bash
# 1. Configurar API key
export OPENAI_API_KEY='your-key'

# 2. Iniciar Streamlit
streamlit run app.py

# 3. Na pÃ¡gina:
#    - 10 samples
#    - âœ“ Usar LLM
#    - 5 samples LLM

# 4. Executar

# 5. Verificar:
#    - Custo ~$0.001
#    - Resultados LLM aparecem
#    - ComparaÃ§Ã£o exibida
```

---

## ğŸ“Š MÃ©tricas de Performance

### Tempos Esperados

| Samples | BERT | LLM (se ativado) | Total |
|---------|------|------------------|-------|
| 10 | ~3s | ~30s | ~33s |
| 50 | ~10s | ~150s | ~160s |
| 100 | ~20s | ~300s | ~320s |
| 500 | ~90s | N/A | ~90s |

### Uso de MemÃ³ria

- **BERT:** ~2GB RAM
- **LLM Judge:** ~500MB RAM
- **VisualizaÃ§Ãµes:** ~100MB RAM

**Total:** ~2.5GB RAM recomendados

---

## ğŸ› Troubleshooting EspecÃ­fico

### Problema: PÃ¡gina nÃ£o aparece

**Causa:** Arquivo nÃ£o estÃ¡ em `frontend/pages/`

**SoluÃ§Ã£o:**
```bash
# Verificar estrutura
ls frontend/pages/

# Deve mostrar:
# 1_ğŸ“_AnÃ¡lise.py
# 2_ğŸ“Š_MÃ©tricas.py
# 3_ğŸ”_Monitoramento.py
# 4_ğŸ§ª_AvaliaÃ§Ã£o.py  â† Este arquivo
```

### Problema: Import Error

**Causa:** MÃ³dulo de evaluation nÃ£o instalado

**SoluÃ§Ã£o:**
```bash
pip install -r requirements-evaluation.txt
```

### Problema: Test data nÃ£o encontrado

**Causa:** Dados nÃ£o preparados

**SoluÃ§Ã£o:**
```bash
python src/data/load_data_v2.py
python src/data/split_dataset.py
```

### Problema: Plotly nÃ£o renderiza

**Causa:** VersÃ£o incompatÃ­vel

**SoluÃ§Ã£o:**
```bash
pip install --upgrade plotly
pip install --upgrade streamlit
```

---

## ğŸ“ Conceitos de UI/UX

### Design Principles

1. **Progressive Disclosure**
   - Mostra opÃ§Ãµes bÃ¡sicas primeiro
   - LLM Ã© opcional e expandÃ­vel

2. **Feedback Imediato**
   - Progress bars em tempo real
   - Loading states
   - Success/error messages

3. **Visual Hierarchy**
   - MÃ©tricas principais em destaque
   - Detalhes em expandibles
   - Cores para guiar atenÃ§Ã£o

4. **Error Prevention**
   - ValidaÃ§Ã£o de inputs
   - Warnings preventivos
   - ConfirmaÃ§Ãµes para aÃ§Ãµes custosas

---

## ğŸš€ PrÃ³ximas Melhorias

### Curto Prazo
- [ ] HistÃ³rico de avaliaÃ§Ãµes
- [ ] ComparaÃ§Ã£o entre runs
- [ ] Export para PDF
- [ ] GrÃ¡ficos adicionais

### MÃ©dio Prazo
- [ ] Agendamento de avaliaÃ§Ãµes
- [ ] NotificaÃ§Ãµes por email
- [ ] Dashboard de trending
- [ ] A/B testing UI

### Longo Prazo
- [ ] Real-time evaluation
- [ ] Collaborative features
- [ ] Mobile app
- [ ] API REST para automaÃ§Ã£o

---

## ğŸ“š Recursos

### DocumentaÃ§Ã£o
- [Streamlit Docs](https://docs.streamlit.io/)
- [Plotly Docs](https://plotly.com/python/)
- [evaluation/README.md](../evaluation/README.md)

### Exemplos
- Ver `main()` function em `4_ğŸ§ª_AvaliaÃ§Ã£o.py`
- Ver outras pÃ¡ginas em `frontend/pages/`

---

## âœ… Checklist de IntegraÃ§Ã£o

- [ ] Arquivo copiado para `frontend/pages/`
- [ ] DependÃªncias instaladas
- [ ] OpenAI API key configurada (opcional)
- [ ] Test data disponÃ­vel
- [ ] Streamlit iniciado
- [ ] PÃ¡gina aparece no menu
- [ ] AvaliaÃ§Ã£o BERT funciona
- [ ] AvaliaÃ§Ã£o LLM funciona (se configurado)
- [ ] VisualizaÃ§Ãµes renderizam
- [ ] Download funciona
- [ ] DocumentaÃ§Ã£o lida

---

## ğŸ‰ ConclusÃ£o

Agora vocÃª tem uma **interface visual completa** para executar avaliaÃ§Ãµes!

**BenefÃ­cios:**
- âœ… Sem necessidade de terminal
- âœ… Interface user-friendly
- âœ… VisualizaÃ§Ãµes profissionais
- âœ… Download automÃ¡tico
- âœ… Tracking de custos

**UsuÃ¡rios podem:**
1. Configurar visualmente
2. Executar com um clique
3. Ver resultados em tempo real
4. Baixar relatÃ³rios
5. Repetir facilmente

**Tudo isso sem digitar um Ãºnico comando!** ğŸ¨

---

**Desenvolvido com â¤ï¸ para o desafio tÃ©cnico de IA SÃªnior**

ğŸ§ª **Testing + ğŸ¨ Frontend = ğŸ’ª User Experience!**
