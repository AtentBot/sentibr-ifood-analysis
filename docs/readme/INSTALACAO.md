# üöÄ FASE 6 - INSTALA√á√ÉO E USO R√ÅPIDO

## üìÅ Estrutura dos Arquivos

Voc√™ recebeu 9 arquivos para a Fase 6:

```
fase6_eval_llm/
‚îú‚îÄ‚îÄ README.md                      # Documenta√ß√£o completa (LEIA PRIMEIRO!)
‚îú‚îÄ‚îÄ __init__.py                    # M√≥dulo Python
‚îú‚îÄ‚îÄ eval_suite.py                  # Framework de avalia√ß√£o
‚îú‚îÄ‚îÄ llm_judge.py                   # LLM-as-a-Judge
‚îú‚îÄ‚îÄ compare_models.py              # Compara√ß√£o BERT vs GPT
‚îú‚îÄ‚îÄ explainability.py              # Explicabilidade (LIME)
‚îú‚îÄ‚îÄ run_evaluation.py              # Script de execu√ß√£o
‚îú‚îÄ‚îÄ requirements-evaluation.txt    # Depend√™ncias
‚îî‚îÄ‚îÄ .env.example                   # Template de configura√ß√£o
```

---

## ‚ö° INSTALA√á√ÉO R√ÅPIDA (3 PASSOS)

### 1Ô∏è‚É£ **Colocar Arquivos no Projeto**

```bash
# No diret√≥rio raiz do projeto SentiBR:

# Criar diret√≥rio de evaluation (se n√£o existir)
mkdir -p src/evaluation

# Copiar m√≥dulos de evaluation
cp eval_suite.py src/evaluation/
cp llm_judge.py src/evaluation/
cp compare_models.py src/evaluation/
cp explainability.py src/evaluation/
cp __init__.py src/evaluation/

# Criar diret√≥rio scripts (se n√£o existir)
mkdir -p scripts

# Copiar script de execu√ß√£o
cp run_evaluation.py scripts/
chmod +x scripts/run_evaluation.py

# Copiar configura√ß√£o
cp .env.example .env
```

### 2Ô∏è‚É£ **Instalar Depend√™ncias**

```bash
# Instalar requirements espec√≠ficos da Fase 6
pip install -r requirements-evaluation.txt

# Ou manualmente:
pip install openai>=1.3.0 lime>=0.2.0.1 tqdm>=4.65.0
```

### 3Ô∏è‚É£ **Configurar OpenAI API Key**

```bash
# Editar .env e adicionar sua chave
nano .env

# Ou export direto
export OPENAI_API_KEY='sk-your-key-here'
```

**‚úÖ Pronto! Instala√ß√£o completa.**

---

## üéØ USO B√ÅSICO

### **Teste R√°pido (sem API Key)**

```bash
# Apenas m√©tricas b√°sicas (n√£o precisa de OpenAI)
python scripts/run_evaluation.py --metrics-only
```

### **Avalia√ß√£o Completa (com API Key)**

```bash
# TODAS as fases (m√©tricas + LLM + compara√ß√£o + explainability)
python scripts/run_evaluation.py --full

# Com menos amostras para economizar (recomendado para testes)
python scripts/run_evaluation.py --full --samples 50
```

### **Fases Individuais**

```bash
# Apenas LLM-as-Judge
python scripts/run_evaluation.py --llm-only --samples 30

# Apenas compara√ß√£o BERT vs GPT
python scripts/run_evaluation.py --comparison --samples 50

# Apenas explicabilidade
python scripts/run_evaluation.py --explainability
```

---

## üìä RESULTADOS

Ap√≥s executar, os resultados estar√£o em:

```
logs/evaluation_YYYYMMDD_HHMMSS/
‚îú‚îÄ‚îÄ metrics/              # M√©tricas + visualiza√ß√µes
‚îú‚îÄ‚îÄ llm_judge/           # Avalia√ß√µes LLM
‚îú‚îÄ‚îÄ bert_vs_gpt/         # Compara√ß√£o de modelos
‚îî‚îÄ‚îÄ explainability/      # Explica√ß√µes LIME
```

---

## üí° EXEMPLOS DE C√ìDIGO

### **Exemplo 1: Avalia√ß√£o B√°sica**

```python
from src.evaluation import ModelEvaluator
import pandas as pd

# Carregar dados
test_df = pd.read_csv('data/processed/test.csv')

# Avaliar
evaluator = ModelEvaluator(model_path='models/bert_finetuned')
results = evaluator.evaluate_dataset(
    df=test_df,
    save_dir='logs/my_evaluation'
)

print(f"Accuracy: {results['metrics']['overall']['accuracy']:.2%}")
```

### **Exemplo 2: LLM-as-Judge**

```python
from src.evaluation import LLMJudge

judge = LLMJudge()  # Usa OPENAI_API_KEY do ambiente

evaluation = judge.evaluate_prediction(
    review_text="Comida √≥tima mas demorou!",
    predicted_sentiment="Positivo",
    predicted_confidence=0.85
)

print(f"Verdadeiro: {evaluation['true_sentiment']}")
print(f"Correto: {evaluation['is_correct']}")
```

### **Exemplo 3: Compara√ß√£o**

```python
from src.evaluation import ModelComparator, GPTSentimentAnalyzer

evaluator = ModelEvaluator(model_path='models/bert_finetuned')
gpt = GPTSentimentAnalyzer()
comparator = ModelComparator(evaluator, gpt)

results = comparator.compare_on_samples(
    reviews=["Excelente!", "P√©ssimo"],
    max_samples=5
)

print(comparator.generate_recommendation(results))
```

---

## üîß TROUBLESHOOTING R√ÅPIDO

### **‚ùå Erro: "OPENAI_API_KEY not found"**

```bash
# Verificar se est√° setada
echo $OPENAI_API_KEY

# Setar novamente
export OPENAI_API_KEY='sk-your-key'

# Ou adicionar ao .env
echo "OPENAI_API_KEY=sk-your-key" > .env
```

### **‚ùå Erro: "Model not found"**

```bash
# Treinar modelo primeiro
python src/training/train.py
```

### **‚ùå Erro: "No module named 'lime'"**

```bash
# Instalar depend√™ncias
pip install -r requirements-evaluation.txt
```

### **‚ùå Rate Limit do OpenAI**

```python
# Reduzir n√∫mero de samples
python scripts/run_evaluation.py --llm-only --samples 10

# Ou aumentar delay no c√≥digo
gpt_df = gpt_analyzer.predict_batch(reviews, delay=1.0)
```

---

## üìà CUSTOS ESTIMADOS

### **OpenAI API (GPT-4o-mini)**

- **Pre√ßo**: ~$0.150 / 1M input tokens
- **Custo por review**: ~$0.000013 (13 tokens m√©dios)
- **100 reviews**: ~$0.0013 (menos de 1 centavo!)
- **1000 reviews**: ~$0.013 (1 centavo!)

### **Recomenda√ß√µes de Uso**

| Samples | Custo Estimado | Tempo | Uso Recomendado |
|---------|----------------|-------|-----------------|
| 10 | $0.0001 | ~30s | Teste r√°pido |
| 50 | $0.0007 | ~2min | Valida√ß√£o |
| 100 | $0.0013 | ~5min | An√°lise padr√£o |
| 500 | $0.0065 | ~25min | An√°lise completa |

---

## ‚úÖ CHECKLIST DE VALIDA√á√ÉO

Execute esta checklist para garantir que tudo est√° funcionando:

```bash
# 1. Verificar instala√ß√£o
python -c "from src.evaluation import ModelEvaluator, LLMJudge; print('‚úÖ Import OK')"

# 2. Verificar API Key
python -c "import os; print('‚úÖ API Key OK' if os.getenv('OPENAI_API_KEY') else '‚ùå API Key Missing')"

# 3. Testar avalia√ß√£o b√°sica (sem API)
python scripts/run_evaluation.py --metrics-only

# 4. Testar LLM (com API) - apenas 5 samples
python scripts/run_evaluation.py --llm-only --samples 5

# 5. Ver resultados
ls -R logs/evaluation_*
```

Se todos os passos funcionarem: **‚úÖ FASE 6 INSTALADA COM SUCESSO!**

---

## üìö DOCUMENTA√á√ÉO COMPLETA

Para mais detalhes, consulte:

- **README.md**: Documenta√ß√£o completa e detalhada
- **C√≥digo**: Todos os arquivos t√™m docstrings e coment√°rios
- **Exemplos**: Ver se√ß√£o "COMO USAR" no README.md

---

## üéØ PR√ìXIMOS PASSOS

Ap√≥s validar a Fase 6:

1. ‚úÖ Testar avalia√ß√£o b√°sica
2. ‚úÖ Testar LLM-as-judge
3. ‚úÖ Testar compara√ß√£o BERT vs GPT
4. ‚úÖ Revisar resultados e m√©tricas
5. ‚û°Ô∏è **FASE 7**: Docker + Docker Compose
6. ‚û°Ô∏è **FASE 8**: Testes (Unit + Integration + Load)

---

## üí¨ SUPORTE

Se tiver d√∫vidas ou problemas:

1. Consulte README.md completo
2. Verifique os docstrings no c√≥digo
3. Execute com --help: `python scripts/run_evaluation.py --help`

---

**‚ú® Boa sorte com a Fase 6!**

**Desenvolvido para o desafio t√©cnico de IA S√™nior** üöÄ
