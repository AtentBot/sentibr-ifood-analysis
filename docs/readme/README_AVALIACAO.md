# üß™ P√°gina de Avalia√ß√£o - Frontend

Interface interativa para executar avalia√ß√µes completas do modelo **sem usar terminal**!

## üéØ Features

### ‚úÖ Avalia√ß√£o BERT
- M√©tricas cl√°ssicas em tempo real
- Confusion Matrix interativa
- An√°lise de erros detalhada
- Valida√ß√£o autom√°tica (threshold 90%)

### ‚úÖ LLM Judge (Opcional)
- Avalia√ß√£o qualitativa com GPT-4o-mini
- Identifica√ß√£o de edge cases
- An√°lise de acordo com BERT
- Tracking de custos em tempo real

### ‚úÖ Compara√ß√£o BERT vs GPT
- Trade-offs side-by-side
- Recomenda√ß√µes de uso
- Estrat√©gia h√≠brida otimizada

### ‚úÖ Download de Relat√≥rios
- Relat√≥rios em JSON
- Resultados salvos automaticamente

---

## üöÄ Como Usar

### 1. Acessar a P√°gina

```bash
# Iniciar Streamlit
cd frontend
streamlit run app.py
```

Depois, clique em **"üß™ Avalia√ß√£o"** no menu lateral.

### 2. Configurar Avalia√ß√£o

**Par√¢metros:**
- **N√∫mero de samples:** 10-1000 (padr√£o: 100)
- **Usar LLM Judge:** Sim/N√£o (requer OpenAI API key)
- **Samples para LLM:** 5-100 (padr√£o: 50)

**Custo estimado** √© exibido automaticamente.

### 3. Executar

Clique em **"üöÄ Executar Avalia√ß√£o Completa"** e aguarde!

### 4. Ver Resultados

A p√°gina exibir√°:

**M√©tricas BERT:**
- Accuracy, Precision, Recall, F1-Score
- Confusion Matrix interativa
- M√©tricas por classe
- An√°lise de erros

**LLM Judge (se ativado):**
- Taxa de acordo com BERT
- Taxa de edge cases
- Confian√ßa m√©dia
- Custo real em USD

**Compara√ß√£o:**
- Vantagens de cada modelo
- Trade-offs detalhados
- Recomenda√ß√£o de estrat√©gia

### 5. Download

Baixe os relat√≥rios em JSON para an√°lise posterior.

---

## üìã Pr√©-requisitos

### Obrigat√≥rio
- Modelo BERT treinado em `models/bert_finetuned/`
- Dados de teste em `data/processed/test.csv`
- M√≥dulo de evaluation instalado

### Opcional (para LLM Judge)
- OpenAI API key configurada
- `export OPENAI_API_KEY='your-key'`

---

## üîß Setup

### 1. Instalar Depend√™ncias

```bash
pip install -r requirements-evaluation.txt
```

### 2. Copiar P√°gina

```bash
# Copiar p√°gina para frontend
cp pages/4_üß™_Avalia√ß√£o.py frontend/pages/
```

### 3. Configurar OpenAI (Opcional)

```bash
# Para usar LLM Judge
export OPENAI_API_KEY='your-api-key'
```

### 4. Verificar Test Data

```bash
# Verificar se test data existe
ls -lh data/processed/test.csv

# Se n√£o existe, criar:
python src/data/load_data_v2.py
python src/data/split_dataset.py
```

---

## üí° Exemplos de Uso

### Caso 1: Avalia√ß√£o R√°pida (BERT apenas)

1. Acessar p√°gina "üß™ Avalia√ß√£o"
2. Configurar: **100 samples**, **Sem LLM**
3. Clicar em "Executar"
4. Ver resultados em ~30 segundos
5. Download do relat√≥rio

**Resultado:** M√©tricas completas sem custos!

### Caso 2: Valida√ß√£o Completa (com LLM)

1. Acessar p√°gina "üß™ Avalia√ß√£o"
2. Configurar: **500 samples BERT**, **50 samples LLM**
3. Verificar custo estimado (~$0.005)
4. Clicar em "Executar"
5. Ver resultados em ~3 minutos
6. Analisar acordo BERT vs LLM
7. Download de ambos os relat√≥rios

**Resultado:** Valida√ß√£o completa com LLM por centavos!

### Caso 3: Monitoramento Cont√≠nuo

**Schedule regular:**
- Segunda, Quarta, Sexta: Avaliar 200 samples
- Verificar se Accuracy > 90%
- Se n√£o, retreinar modelo

---

## üìä Interpretando Resultados

### ‚úÖ Modelo Aprovado
```
Accuracy: ‚â• 90%
F1-Score: ‚â• 88%
LLM Agreement: ‚â• 85%
```
**A√ß√£o:** Deploy em produ√ß√£o ‚úÖ

### ‚ö†Ô∏è Modelo OK
```
Accuracy: 85-90%
F1-Score: 83-88%
LLM Agreement: 75-85%
```
**A√ß√£o:** Considere melhorias antes de deploy

### ‚ùå Modelo Precisa Melhoria
```
Accuracy: < 85%
F1-Score: < 83%
LLM Agreement: < 75%
```
**A√ß√£o:** Retreine o modelo com mais dados

---

## üí∞ Custos do LLM Judge

| Samples | Tokens | Custo USD | Uso |
|---------|--------|-----------|-----|
| 10 | ~3k | $0.001 | Smoke test |
| 50 | ~15k | $0.005 | Quick validation |
| 100 | ~30k | $0.015 | Standard eval |

**Dica:** Use 50-100 samples para valida√ß√£o regular.

---

## üé® Interface

### Tela Principal

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üß™ Avalia√ß√£o do Modelo                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  ‚öôÔ∏è Configura√ß√£o da Avalia√ß√£o              ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  [N√∫mero de samples]  [100]                 ‚îÇ
‚îÇ  [‚úì] Usar LLM Judge                        ‚îÇ
‚îÇ  [Samples LLM]       [50]                  ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  üí∞ Custo estimado: $0.005                 ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  [üöÄ Executar Avalia√ß√£o Completa]          ‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Resultados

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üìä M√©tricas do BERT                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  92.3%      91.8%      92.0%      91.9%     ‚îÇ
‚îÇ  Accuracy   Precision  Recall    F1-Score   ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚è±Ô∏è Tempo: 28.5s                            ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  [Confusion Matrix interativa]             ‚îÇ
‚îÇ  [M√©tricas por classe]                     ‚îÇ
‚îÇ  [An√°lise de erros]                        ‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ü§ñ LLM Judge (GPT-4o-mini)                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  87.5%      12.0%      0.89       $0.005    ‚îÇ
‚îÇ  Acordo     Edge Cases  Confian√ßa  Custo    ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  ‚è±Ô∏è Tempo: 156.2s | üéØ 50 samples          ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ  [Distribui√ß√£o de sentimentos]             ‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  üÜö Compara√ß√£o BERT vs LLM Judge            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                             ‚îÇ
‚îÇ  [Vantagens de cada] [Trade-offs]          ‚îÇ
‚îÇ  [Recomenda√ß√£o: Estrat√©gia H√≠brida]        ‚îÇ
‚îÇ                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üêõ Troubleshooting

### Problema 1: "M√≥dulo de avalia√ß√£o n√£o encontrado"

**Solu√ß√£o:**
```bash
pip install -r requirements-evaluation.txt
```

### Problema 2: "OpenAI API Key n√£o configurada"

**Solu√ß√£o:**
```bash
export OPENAI_API_KEY='your-key'
# Reiniciar Streamlit
```

### Problema 3: "Dados de teste n√£o encontrados"

**Solu√ß√£o:**
```bash
python src/data/load_data_v2.py
python src/data/split_dataset.py
```

### Problema 4: P√°gina n√£o aparece no menu

**Solu√ß√£o:**
```bash
# Verificar arquivo
ls frontend/pages/4_üß™_Avalia√ß√£o.py

# Se n√£o existe, copiar
cp pages/4_üß™_Avalia√ß√£o.py frontend/pages/

# Reiniciar Streamlit
```

---

## üéØ Best Practices

### Frequ√™ncia de Avalia√ß√£o

**Desenvolvimento:**
- Ap√≥s cada treinamento
- Antes de cada deploy

**Produ√ß√£o:**
- Semanal: 200-500 samples
- Mensal: 1000+ samples

### Otimiza√ß√£o de Custos

1. **BERT Primeiro:** Sempre execute BERT (gr√°tis)
2. **LLM Estrat√©gico:** Use 50-100 samples, n√£o todos
3. **Sample Inteligente:** Foque em casos de baixa confian√ßa
4. **Cache Resultados:** Salve relat√≥rios para an√°lise posterior

### Valida√ß√£o de Produ√ß√£o

Antes de deploy, certifique-se:
- ‚úÖ Accuracy ‚â• 90%
- ‚úÖ F1-Score ‚â• 88%
- ‚úÖ Todas as classes com F1 ‚â• 0.85
- ‚úÖ LLM Agreement ‚â• 85% (se testado)
- ‚úÖ Sem erros graves em edge cases

---

## üìà Pr√≥ximos Passos

Ap√≥s valida√ß√£o bem-sucedida:

1. **Deploy:** Colocar modelo em produ√ß√£o
2. **Monitor:** Configurar monitoring cont√≠nuo
3. **Feedback Loop:** Coletar feedback de usu√°rios
4. **Retreino:** Agendar retreino peri√≥dico

---

## üéì Conceitos

### Por que Avaliar?

**Antes do Deploy:**
- Garantir qualidade m√≠nima
- Identificar pontos fracos
- Comparar vers√µes

**Durante Produ√ß√£o:**
- Detectar degrada√ß√£o
- Identificar drift
- Validar melhorias

### BERT vs LLM Judge

**Use BERT quando:**
- Precisa de velocidade
- Quer zero custos
- Tem volume alto

**Use LLM quando:**
- Precisa explica√ß√£o
- Quer valida√ß√£o qualitativa
- Budget permite

**Use Ambos quando:**
- Valida√ß√£o cr√≠tica
- Identificar edge cases
- Treinar novo modelo

---

## üéâ Conclus√£o

Com esta p√°gina, voc√™ pode:

‚úÖ **Avaliar modelos** sem terminal  
‚úÖ **Visualizar resultados** em tempo real  
‚úÖ **Comparar BERT vs GPT** interativamente  
‚úÖ **Download relat√≥rios** automaticamente  
‚úÖ **Monitorar custos** em tempo real  

**Interface 100% visual e user-friendly!** üöÄ

---

**Desenvolvido com ‚ù§Ô∏è para o desafio t√©cnico de IA S√™nior**

üß™ **Testing + üé® UI = üí™ Professional ML!**
